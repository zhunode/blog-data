{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该小节主要探讨模型训练的好不好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In machine learning,the goals is to achieve models that generalize--that perform well on nerver-before-seen data--and overfitting is the central obstacle.\n",
    "+ You can only control that which you can observe,so it's crucial to be able to reliably measure the generalization power of your model.\n",
    "+ The following sections look at strategies for mitigating overfitting and maximizing generalization.In this section,we'll focus on how to measure generalization:how to evaluate machine-learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training,validation,and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Evaluating a model always boils down to splitting the available data into three sets:training,validation,and test.\n",
    "+ You may ask ,why not have two sets:a training set and a test set?The reason is that developing a model always involves tuning its configuration:for example,choosing the number of layers or the size of the layers.You do this tuning by using as a feedback signal the performance of the model on the validation data.In essence,this tuning is a form of learning.\n",
    "+ Central to this phenomenon is the notion of information leaks.Every time you tune a hyperparameter of your model based on the model's performance on the validation sets,some information abount the validation data leaks into the model.\n",
    "+ Let's review three classic evaluation recipes:simple hold-out validation,K-fold validation,and iterated,and iterated K-fold validation with shuffling.\n",
    "\n",
    "模型训练中若只有训练集和测试集的话，这样会导致信息泄露的问题。意思是当我们用训练数据对模型训练后，就直接用测试集来测试模型效果怎样，如果效果不好的化，那么就需要调参继续用训练集来训练模型，继而在用测试集来测试模型效果，这样往返多次，就会导致测试集信息泄露。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMPLE HOLD-OUT VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This is the simplest evaluation protocol,and it suffers from one flaw:if little data is availabel,then your validation and test sets may contain too few samples to be statistically representative of the data at hand.\n",
    "\n",
    "缺点：验证机和测试集数量太小的化，无法满足整体效果。\n",
    "\n",
    "+ This is easy to recognize:if different random shuffling rounds of the data before splitting end up yielding very different measures of model performance,then you're having this issue.\n",
    "\n",
    "数据集太小，可能会导致shuffle之后，数据集的效果相差较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This method is helpful when the performances of your model shows significant variance based on your traintest split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITERATED K-FOLD VALIDATION WITH SHUFFLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ It consists of applying K-fold validation multiple times,shuffling the data every time before splitting it K ways.\n",
    "+ The final score is the average of the scores obtained at each run of K-fold validation.\n",
    "+ Note that you end up training and evaluating  P*K models(where P is the number of iterations you use),which can very expensive.\n",
    "\n",
    "当数据量是比较少的情况下，那么就用 带有打乱数据的迭代交叉验证方法 来进行模型训练。\n",
    "\n",
    "为什们有效呢？因为数据量少，无法有效的训练模型。经过shuffle之后，对数据进行切分，那么每次的训练、验证和测试数据都不同，对于模型而言，每次数据集不同，可以对模型有改善效果，同时也避免了过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to keep in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Data representativeness -- You usually should randomly shuffle your data before splitting it into training and test sets.\n",
    "+ The arrow of time -- If you're trying to predict the future given the past(for example,tomorrow's weather,stock movements,and so on),you should not randomly shuffle your data before splitting it,because doing so will create a temporal leak.\n",
    "+ Redunancy in your data -- If some data points in your data appear twice(fairly common with real-world data),then shuffling the data and splitting it into a training set and a validation set will result in redundancy between the training and validation sets.Make sure your training set and validation set are disjoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
