{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defineing the problem and assembliing a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ What will your input data be ? What are you trying to predict?You can only learn to predict something if you have a training data.\n",
    "+ What type of problem are you facing? Is it binary classification?Multiclass classification?Something else , like clustering,generation,or reinforcement learning?\n",
    "\n",
    "\n",
    "+ Using machine learning trained on past data to predict the future is making the assumption that the future will behave like the past.That ofen isn't the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a measure of success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ For balanced-classification problems,where every class is eqully likely,accuracy and area under the receiver operating characteristic curve(ROC AUC) are common metrics.\n",
    "\n",
    "+ For class-imblanced problems,you can use precision and recall.\n",
    "\n",
    "+ For ranking problem or multilabel classification,you can use mean average precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding on an evaluation protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Maintaining a hold-out validation set--The way to go when you have plenty of data.\n",
    "+ Doing K-fold cross-validation--The right choice when you have too few samples for hold-out validation to be reliable.\n",
    "+ Doing iterated K-fold validation--For performing highly accurate model evaluation when little data is availabel Just pick one of these.In most cases,the first will work well enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Your data should be formatted as tensors.\n",
    "+ The values taken by these tensors should usually be scaled to small values.\n",
    "+ If different features take values in different ranges(heterogeneous data),then the data should be normalized.\n",
    "+ You may want to do some feature engineering,especially for small-data problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing a model that does better than a baseline\n",
    "\n",
    "baseline : 基线，就是开发的模型的效果有比随便乱猜的要好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Your goall at this stage is to achieve statistical power:that is ,to develop a small model that is capable of beating a baseline.\n",
    "+ If you can't beat a random baseline after trying multiple reasonable architectures,it may be that the answer to the question you're asking isn't present in the input data.\n",
    "+ Assuming that things go well,you need to make three key choices to build your first working model:Last-layer activation,Loss function,and Optimization configuration.\n",
    "+ Note that it isn't always possible to directly optimizer for the metric that measures success on a problem.For instance ,the widely used classification metric ROC AUC can't be directly optimized.\n",
    "+ Choose the right last-layer activation and loss function:See the followings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Problem type/Last-layer activation/Loss function:\n",
    "\n",
    "<img src=\"./images/choose_type.png\"/>\n",
    "\n",
    "+ Binary classification / sigmoid /binary_crossentropy\n",
    "+ Multiclass,single-label classification /softmax/categorical_crossentropy\n",
    "+ Multiclass,multilabel classification / sigmoid /binary_crossentropy\n",
    "+ Regression to arbitrary values / None / mse\n",
    "+ Regression to values between 0 and 1 / sigmoid / mse or binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up :developing a model that overfits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ To figure out how big a model you'll need,you must develop a model that overfits.\n",
    "+ When you see that the model's performance on the validation data begins to degrade,you're achieved overfitting.\n",
    "+ The next stage is to start regularizing and tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing your model and tuning your hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some things you should try:\n",
    "+ Add dropout\n",
    "+ Try different architectures:add or remove layers\n",
    "+ Add L1 and/or L2 regularization\n",
    "+ Try different hyperparameters(such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration.\n",
    "+ Optionally,iterate on feature engineering:add new features, or remove features that don't seem to be informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Once you're developed a satisfactory model configuration,you can train your final production model on all the available data(training and validation) and evaluate it one last time on the test set.\n",
    "\n",
    "+ If if turns out that performance on the test set is significantly worse than the performance measured on the validation data,this may mean either that your validation procedure wasn't reliable after all,or that you began overfitting to the validation data while tuning the parameters of the model.In this case,you may want to switch to a more reliable evaluation protocol(such as iterated K-fold validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
