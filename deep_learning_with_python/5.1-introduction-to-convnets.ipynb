{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.version)\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#     raise SystemError(\"GPU device not found\")\n",
    "# print(\"Found GPU AT:{}\".format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:166: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:180: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# tensorflow2.0设置\n",
    "# config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "# config.gpu_options.allow_growth = True\n",
    "# tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "# tensorflow>2.0的设置\n",
    "# keras的设置如下\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "keras.backend.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5.1 - Introduction to convnets\n",
    "\n",
    "This notebook contains the code sample found in Chapter 5, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "First, let's take a practical look at a very simple convnet example. We will use our convnet to classify MNIST digits, a task that you've already been \n",
    "through in Chapter 2, using a densely-connected network (our test accuracy then was 97.8%). Even though our convnet will be very basic, its \n",
    "accuracy will still blow out of the water that of the densely-connected model from Chapter 2.\n",
    "\n",
    "The 6 lines of code below show you what a basic convnet looks like. It's a stack of `Conv2D` and `MaxPooling2D` layers. We'll see in a \n",
    "minute what they do concretely.\n",
    "Importantly, a convnet takes as input tensors of shape `(image_height, image_width, image_channels)` (not including the batch dimension). \n",
    "In our case, we will configure our convnet to process inputs of size `(28, 28, 1)`, which is the format of MNIST images. We do this via \n",
    "passing the argument `input_shape=(28, 28, 1)` to our first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:504: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3828: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3652: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "'''\n",
    "卷积层\n",
    "第一层卷积层定义了32个滤镜，每个滤镜的大小是3*3，这样经过滤镜处理后的图片大小为：\n",
    "    (28 - 3 + 1)/1(stride=1) = 0\n",
    "    (width/height - filter + 1 + p)/s\n",
    "\n",
    "最后经过滤镜得到的结果用relu进行非线性转换。\n",
    "关于Conv2D方法的用法可以查看官方文档。\n",
    "参数量计算为：\n",
    "    第一层的卷积核为：3*3，每一个卷积核的参数为：3*3*上一层的通道数+biase项=3*3*1+1\n",
    "    第一层的channel有32个，所以所需的参数为:每一个卷积核参数大小乘以当前层的channel = (3*3*1+1)*32=320\n",
    "    从而第一层的总共参数大小为320个。\n",
    "    \n",
    "'''\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "'''\n",
    "maxpooling : 池化层\n",
    "    第一层池化层大小为(2,2)，这样经过池化后的图片大小为：(26/2,26/2)=(13,13)\n",
    "    从而经过池化后的图片大小变为了(13,13,32):意思是有32个通道的(13,13)大小的图片\n",
    "'''\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "'''\n",
    "一个滤镜所需要的参数量为：滤镜大小为3*3，上一层滤镜数为32，偏置项个数1。从而一个滤镜所需要\n",
    "的参数量为3*3*32+1,现在该卷积层有64个滤镜，从而需要的总参数量为(3*3*32+1)*64\n",
    "\n",
    "    图片(width,hegiht) = ((13-3+1)/1,(13-3+1)/1)=(11,11)\n",
    "'''\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "'''\n",
    "第二层池化层，\n",
    "    图片(width,height) = (11/2,11/2) = (5,5) 向下取整\n",
    "'''\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# 最后一层卷积需要的参数大小为：(3*3*64+1)*64\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "?layers.Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "?layers.MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the architecture of our convnet so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see above that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width, channels)`. The width \n",
    "and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to \n",
    "the `Conv2D` layers (e.g. 32 or 64).\n",
    "\n",
    "The next step would be to feed our last output tensor (of shape `(3, 3, 64)`) into a densely-connected classifier network like those you are \n",
    "already familiar with: a stack of `Dense` layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. \n",
    "So first, we will have to flatten our 3D outputs to 1D, and then add a few `Dense` layers on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将图片摊平，变为一维\n",
    "model.add(layers.Flatten())\n",
    "# 该层是fully connect\n",
    "# 参数计算为：（1*1*576+1）*64=36928,可以认为全连接层的卷积核大小为1*1。\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# 该层也是fully connect,最后输出\n",
    "# 参数计算为：(1*1*64+1)*10=650\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation. Now here's what our network \n",
    "looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our `(3, 3, 64)` outputs were flattened into vectors of shape `(576,)`, before going through two `Dense` layers.\n",
    "\n",
    "Now, let's train our convnet on the MNIST digits. We will reuse a lot of the code we have already covered in the MNIST example from Chapter \n",
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# 图片需要变为4维张量\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# one-hot encoding to_categorical相当于one-hot encoding.\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\optimizers.py:744: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 20s 339us/step - loss: 0.1802 - acc: 0.9432\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 20s 329us/step - loss: 0.0466 - acc: 0.9853\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 20s 334us/step - loss: 0.0315 - acc: 0.98985s - loss: 0.0320 - - ETA:  - ETA: 2s - - ETA: 1s - loss: 0\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 0.0234 - acc: 0.9924\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 19s 325us/step - loss: 0.0179 - acc: 0.9941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27051573518>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 84us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9914"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our densely-connected network from Chapter 2 had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.3%: we \n",
    "decreased our error rate by 68% (relative). Not bad! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The fundamental difference between a densely connected layer and a convolution layer is this:Dense layers learn global patterns in their input feature space(for example,for a MNIST digit,patterns involving all pixels),whereas convolution layers learn local patterns.\n",
    "\n",
    "+ This key characteristic gives convnets two interesting properties.\n",
    ">+ The patterns they learn are translation invariant.After learning a certain pattern in the low-right corner of a picture,a convnet can recognize it anywhere:for example,in the upper-left corner.A densely connected network would have to learn the pattern anew if it appeared at a new location.This makes convnets data efficient when processing images(because the visual world is fundamentally translation invariant):they need fewer training samples to learn representations that have generalization power.\n",
    ">+ They can learn spatial hierarchies of patterns.A first convolution layer will learn small local patterns such as edges,a second convolution layer will learn larger patterns made of the features of the first layers,and so on.This allows convnets to efficiently learn increasingly complex and abstract visual concepts(because the visual world is fundamentally spatially hierachical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convolutions operate over 3D tensors,called feature maps,with two spatial axes(height and width) as well as a depth axis(also called the channels axis).\n",
    "+ The convolution operation extracts patches from its input feaure map and applies the same transformation to all of these patches producing an ouput feature amp.This ooutput feature map is still a 3D tensor:It has a width and a height.\n",
    "+ Its depth can be arbitrary,because the output depth is a parameter of the layer,and the current channels in that depth axis no longer stand for specific colors as in RGB input;rather,they stand for filters.\n",
    "+ In the MNIST examples,the first convolution layer takes a feature map of size(28,28,1) and outputs a feature map of size(26,26,32);it computes 32 filters over its input.Each of these 32 output channels contains a 26*26 grid of values,which is a response map f the filter over the input,indicating the response of that filter pattern at different locations in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convolutions are defined by two key parameters:\n",
    ">+ Size of the patches extracted from the inputs--These are typically 3*3 or 5*5.In the example,they were 3*3,which is a common choice.\n",
    ">+ Depth of the ouotput feature map--The number of filters computed by the convolution.The example started with a depth of 32 and ended with a depth of 64.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Note that the output width and height may differ from the input width and hegiht.They may differ for two reasons:\n",
    ">+ Border effects,which can be countered by padding the input feature map.In Conv2D layers,padding is configurable via the padding argument,which takes two values:\"valid\",which means no padding(only valid window locations will be used);and \"same\",which means \"pad in such a way as to have an output with the same width and height as the input.\"The padding argument defaults to \"valid\".\n",
    ">+ The use of stride.The distance between two successive windows is a paramter of the convolution,called its stride,which defaults to 1.It's possible to have strided convolutions:convolutions with a stride higher than 1.Using stride 2 means the width and height of the feature map are downsampled by a factor 2((in addition to any changes induced by border effects).Strided convolutions are rarely used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The max-pooling operation\n",
    "+ The role of max pooling:to aggressively downsample feature maps,much like strided convolutions.\n",
    "+ It's conceptually similar to convolutin,except thath instead of transforming local patches via a learned linear transformation,they're transformed via a hardcoded max tensor operation.\n",
    "+ A big difference from convolutions is that max pooling is usually done with 2*2 windows and stride 2,in order to downsample the feature maps by a factor of 2.On the other hand,convolutions is typically done with 3*3 windows and no stride(stride 1).\n",
    "+ Why downsample feature maps this way?Why not remove the max-pooling layers and keep fairly large feature maps all the way up?Let's look at this option.The convolutional base of the model would then look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_max_pool = models.Sequential()\n",
    "model_no_max_pool.add(layers.Conv2D(32,(3,3),activation='relu',\n",
    "                                    input_shape=(28,28,1)))\n",
    "model_no_max_pool.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model_no_max_pool.add(layers.Conv2D(64,(3,3),activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_max_pool.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ What's wrong with this setup?Two things:\n",
    ">+ It isn't conductive to learning a spatial hierachy of features.The 3*3 windows in the third layer will only contain information coming from 7*7 windows in the initial input.The hihg-level patterns learned bby the convnet will still be very small with regard to the initial input,which may not be ennough to learn to classify digits.We need the features from the last convolution layer to contain information about the totality of the input.\n",
    ">+ The final feature map has 22*22*64=30976 total coefficients per sample.This is huge.If you were to flattern it to stick a Dense layer of size 512 on top,that layer would have 15.8 million parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In short,the reason to use downsampling is to reduce the number of eature-map coefficients to process,as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows(in terms of the fraction of the original input they cover).\n",
    "+ You can use average pooling instead of max pooling,where each local input patch is transfomed by taking the average value of each channel over the patch,rather than the max.But max pooling tends to work better than these alternative solutions.\n",
    "+ In a nutshell,the reason is that features tend to encode the spatial presence of some pattern or concept over the different tiles of the feature map(hence,them feature map),and it's more information to look at the maximal presence of different features than at their average presence.\n",
    "+ The most reasonable subsampling strategy is to first produce dense maps of features(via unstrided convolutions) and then look at the maximal activation of the features over small patches,rather than looking at sparser windows of the inputs(via strided convolutions) or averaging input ptaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
