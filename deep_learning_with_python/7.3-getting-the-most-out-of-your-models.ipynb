{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Normalizing Neural Networks 自带正则化的神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 让模型变得更好\n",
    "\n",
    "一般深度学习喜欢比较小的数值，所以一般我们都会对数据进行标准化操作，让数值变得更小。\n",
    "\n",
    "该节视频：https://www.bilibili.com/video/av85562236?p=54\n",
    "https://www.bilibili.com/video/av85562236?p=55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "\n",
    "可以解决sigmoid梯度平缓的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.add(layers.Conv2D(32,3,activation='relu'))\n",
    "'''\n",
    "经过标准化操作后，x值变小，求得的梯度变大，更新变快\n",
    "比如sigmoid而言，x变小后，那么它的梯度就会变得合适，更新变得更快。\n",
    "\n",
    "BatchNormalization是如何计算标准差的呢？因为BatchNormalization()是批次加入数据的，\n",
    "无法对全部的数据进行计算标准差，所以呢，BatchNormalization的做法是：\n",
    "对逐步加入的批次计算标准差，然后根据指数平滑来求得每加一笔之后的标准差。\n",
    "\n",
    "训练数据时可以这做BatchNormalization，测试的时候就不需要做BatchNormalization，\n",
    "因为参数都已经计算好了,没有必要做标准化。\n",
    "\n",
    "\n",
    "'''\n",
    "conv_mode.add(layers.BatchNormalization()) \n",
    "\n",
    "dense_model.add(layers.Dense(32,activation='relu'))\n",
    "dense_model.add(layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(train_images,train_labels),(test_images,test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000,28*28))\n",
    "train_images = train_images.astype(\"float32\")/255\n",
    "\n",
    "test_images = test_images.reshape((10000,28*28))\n",
    "test_images = test_images.astype('float32')/255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:504: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3828: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512,activation='relu',input_shape=(28*28,)))\n",
    "model.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 9s 187us/step - loss: 0.0338 - acc: 0.9891 - val_loss: 0.0942 - val_acc: 0.9746\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 8s 169us/step - loss: 0.0225 - acc: 0.9925 - val_loss: 0.0801 - val_acc: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c0f32f8198>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images,train_labels,\n",
    "         epochs=2,\n",
    "         batch_size=32,\n",
    "         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 24us/step\n",
      "[0.07902216555672785, 0.9772]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images,test_labels,verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 2.3337 - acc: 0.1023 - val_loss: 2.3307 - val_acc: 0.1081\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 69s 1ms/step - loss: 2.3288 - acc: 0.1031 - val_loss: 2.3325 - val_acc: 0.1060\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "[2.3297152267456056, 0.1135]\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512,activation='sigmoid',input_shape=(28*28,)))\n",
    "\n",
    "# 这里就是sigmoid的平缓梯度导致的模型效果不好\n",
    "for i in range(9):\n",
    "    model.add(layers.Dense(512,activation='sigmoid'))\n",
    "\n",
    "model.add(layers.Dense(10,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images,train_labels,\n",
    "         epochs=2,\n",
    "         batch_size=32,\n",
    "         validation_split=0.2)\n",
    "score = model.evaluate(test_images,test_labels,verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 75s 2ms/step - loss: 0.5698 - acc: 0.8398 - val_loss: 0.3161 - val_acc: 0.9192\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 73s 2ms/step - loss: 0.3575 - acc: 0.9024 - val_loss: 0.2012 - val_acc: 0.9468\n",
      "10000/10000 [==============================] - 1s 122us/step\n",
      "[0.21457544406056403, 0.943]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "model = models.Sequential()\n",
    "# 首先这里定义了512个神经元\n",
    "model.add(layers.Dense(512,activation='sigmoid',input_shape=(28*28,)))\n",
    "'''\n",
    "在这里为什么参数量是512*4 = 2048。\n",
    "首先做标准化操作是需要均值和标准差两个值，\n",
    "512是指上一层的512个神经元，每个神经元有均值和标准差两个值，从而需要\n",
    "参数量为1024,但是BatchNormalization()会在此均值和标准差上做调整，调整到\n",
    "生成新的均值和标准差，从而再需要*2 = 2048.\n",
    "'''\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 经过BatchNormalization后，效果又好了。\n",
    "for i in range(9):\n",
    "    model.add(layers.Dense(512,activation='sigmoid'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "model.add(layers.Dense(10,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images,train_labels,\n",
    "         epochs=2,\n",
    "         batch_size=32,\n",
    "         validation_split=0.2)\n",
    "score = model.evaluate(test_images,test_labels,verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 2,789,386\n",
      "Trainable params: 2,780,170\n",
      "Non-trainable params: 9,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xception:Depthwise Separable Convolution \n",
    "\n",
    "\n",
    "空间相关性和通道相关性一起做的话，参数估计个数为：\n",
    "    (filter_width,filter_height,last_channel_num)*current_channel_num\n",
    "  \n",
    "空间相关性和通道相关性分开来做的话：\n",
    "\n",
    "    先抓空间相关性：(考虑深度的filter,抓的是空间！)\n",
    "\n",
    "        (filter_width,fitler_height,last_channel_num)\n",
    "\n",
    "    再在抓通道的相关性(1 * 1大小的矩阵，只能考虑通道之间的相关性，并无空间的说法。)：\n",
    "\n",
    "        (1,1,last_channel_num)*current_channel_num\n",
    "\n",
    "        这里1 * 1主要考虑的就是通道相关性，而忽略空间相关性。\n",
    "\n",
    "        1*1的滤镜有32层，现在需要估计64个滤镜，从而需要的参数量为1*1*32*64。\n",
    "\n",
    "    按道理空间相关性应该是包含了通道相关性的把？\n",
    "    \n",
    " \n",
    "若想了解Xception长什么样呢？可以google图片。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras import layers\n",
    "\n",
    "height=64\n",
    "width = 64\n",
    "channel = 3\n",
    "num_classes = 10\n",
    "\n",
    "model = Sequential()\n",
    "'''\n",
    "该层的参数个数大小为：\n",
    "空间相关性参数个数       通道相关性的参数个数(这里加上了bias项)\n",
    "     3*3*3           +     (1+1*1*3)*32\n",
    "'''\n",
    "model.add(layers.SeparableConv2D(32,3,activation='relu',\n",
    "                                 input_shape=(height,width,channel,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3652: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "该层的参数个数大小为：\n",
    "空间相关性参数个数       通道相关性的参数个数\n",
    "     3*3*64           +     (1+1*1*64)*64\n",
    "'''\n",
    "model.add(layers.SeparableConv2D(64,3,activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.SeparableConv2D(64,3,activation='relu'))\n",
    "model.add(layers.SeparableConv2D(128,3,activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.SeparableConv2D(64,3,activation='relu'))\n",
    "model.add(layers.SeparableConv2D(128,3,activation='relu'))\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dense(num_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "separable_conv2d_1 (Separabl (None, 62, 62, 32)        155       \n",
      "_________________________________________________________________\n",
      "separable_conv2d_2 (Separabl (None, 60, 60, 64)        2400      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_3 (Separabl (None, 28, 28, 64)        4736      \n",
      "_________________________________________________________________\n",
      "separable_conv2d_4 (Separabl (None, 26, 26, 128)       8896      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_5 (Separabl (None, 11, 11, 64)        9408      \n",
      "_________________________________________________________________\n",
      "separable_conv2d_6 (Separabl (None, 9, 9, 128)         8896      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 38,949\n",
      "Trainable params: 38,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images,train_labels),(test_images,test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000,28,28,1))\n",
    "train_images = train_images.astype(\"float32\")/255\n",
    "\n",
    "test_images = test_images.reshape((10000,28,28,1))\n",
    "test_images = test_images.astype(\"float32\")/255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Users\\junode\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 40s 829us/step - loss: 0.3651 - acc: 0.8809 - val_loss: 0.1197 - val_acc: 0.9624\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 37s 777us/step - loss: 0.0925 - acc: 0.9713 - val_loss: 0.0825 - val_acc: 0.9759\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 37s 780us/step - loss: 0.0623 - acc: 0.9811 - val_loss: 0.0648 - val_acc: 0.9815\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 40s 832us/step - loss: 0.0478 - acc: 0.9852 - val_loss: 0.0524 - val_acc: 0.9848\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 38s 798us/step - loss: 0.0384 - acc: 0.9882 - val_loss: 0.0522 - val_acc: 0.9847\n",
      "10000/10000 [==============================] - 3s 260us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9869"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model,Sequential\n",
    "from keras import layers\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "num_classes = 10\n",
    "model = Sequential()\n",
    "model.add(layers.SeparableConv2D(32,3,activation='relu',\n",
    "                                 input_shape=(height,width,channels)))\n",
    "model.add(layers.SeparableConv2D(64,3,activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.SeparableConv2D(64,3,activation='relu'))\n",
    "model.add(layers.SeparableConv2D(128,3,activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dense(num_classes,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images,train_labels,batch_size=32,validation_split=0.2,epochs=5)\n",
    "\n",
    "test_loss,test_acc = model.evaluate(test_images,test_labels)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "separable_conv2d_17 (Separab (None, 26, 26, 32)        73        \n",
      "_________________________________________________________________\n",
      "separable_conv2d_18 (Separab (None, 24, 24, 64)        2400      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_19 (Separab (None, 10, 10, 64)        4736      \n",
      "_________________________________________________________________\n",
      "separable_conv2d_20 (Separab (None, 8, 8, 128)         8896      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                65568     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 82,003\n",
      "Trainable params: 82,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.1436 - acc: 0.9542 - val_loss: 0.0721 - val_acc: 0.9800\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 73s 2ms/step - loss: 0.0425 - acc: 0.9873 - val_loss: 0.0536 - val_acc: 0.9877\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 74s 2ms/step - loss: 0.0304 - acc: 0.9910 - val_loss: 0.0563 - val_acc: 0.9851\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 72s 2ms/step - loss: 0.0243 - acc: 0.9934 - val_loss: 0.0502 - val_acc: 0.9882\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 73s 2ms/step - loss: 0.0197 - acc: 0.9944 - val_loss: 0.0453 - val_acc: 0.9900\n",
      "10000/10000 [==============================] - 3s 264us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9918"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用普通的rnn来训练mnist\n",
    "\n",
    "from keras.models import Model,Sequential\n",
    "from keras import layers\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "num_classes = 10\n",
    "model = Sequential()\n",
    "model.add(layers.Conv2D(32,3,activation='relu',\n",
    "                                 input_shape=(height,width,channels)))\n",
    "model.add(layers.Conv2D(64,3,activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.Conv2D(64,3,activation='relu'))\n",
    "model.add(layers.Conv2D(128,3,activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dense(num_classes,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images,train_labels,batch_size=32,validation_split=0.2,epochs=5)\n",
    "\n",
    "test_loss,test_acc = model.evaluate(test_images,test_labels)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The architecture-level parameters are called hypterparameters to distinguish them from the parameters of a model,which are trained via backpropagation.\n",
    "\n",
    "+ It shoundn't be your job as a human to fiddel with hyperparameters all day--that is better left to a machine.Thus you need to explore the space of possible decision automatically,systematically,in a principied way.You need to search the architecture space and find the bestperforming ones empirically.That's whath the field of automatic hyperparameter optimization is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The process of optimizing hyperparameters typically looks like this:\n",
    ">+ Choose a set of hyperparameters(automatically).\n",
    ">+ Build the corresponding model.\n",
    ">+ Fit it to your training data,and measure the final performance on the validation data.\n",
    ">+ Choose the next set of hyperparameters to try(automatically)\n",
    ">+ Eventually,measure performance on your test data.\n",
    "\n",
    "+ The key to this process is the algorithm that uses this history of validation performance,given various sets of hyperparameters,to choose the next set of hyperparameters to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Updating hyperparameters,on the other hand,is extremely challenging,Consider the following:\n",
    ">+ Computing the feedback signal(does this set of hyperparameters lead to a high-performing model on this task?) can be extremely expensive:it requires creating and training a new model from scratch on your dataset.\n",
    ">+ The hyperparameter space is typically made of discrete decisions and thus isn't continuous or differentiable.Hence,you typically can't do gradient descent in hyperparamter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Often,it turns out that rando search(choosing hyperparamters to evaluate at random,repeatedly) is the best solution,despite being the most navie one.\n",
    "+ One tool reliably better than random search is `Hyperopt`,a Python library for hyperparamter optimization that internally uses tress of Parzen estimators to predict sets of hyperparamters that are likely work well.\n",
    "+ Another library called `Hyperas` integrates Hyperas for use with Keras models.\n",
    "+ NOTE:One important issue to keep iin mind when doing automatic hyperparameter optimization at scale is validation-set overfitting.Because you're updating hyperparamters baesed on a signal that is computed using your validation data,you're effectively traning them on the validation data,and thus they will quickly overfit to the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperas demo\n",
    "https://www.bilibili.com/video/av85562236?p=55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ If you look at machine-learning competitions,in particular on Kaggle,you'll see that the winners use very large ensembles of models that inevitaly beat any single model,no matter how good.\n",
    "+ Ensembling relies on the assumption that different good models trained independently are likely to be good for different reason:each model looks at slightly different aspects of the data to make its predictions,getting part of the \"truth\" but not all of it.By pooling their perspectives together,you can get a far more accurate description of the data.\n",
    "+ The easiest way to pool the predictions of a set of classifiers(to ensemble the classifiers) is to average their predictions.This willl work only if the classifiers are more or less equally good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preds_a = model_a.predict(x_val)\n",
    "preds_b = model_b.predict(x_val)\n",
    "preds_c = model_c.predict(x_val)\n",
    "preds_d = model_d.predict(x_val)\n",
    "\n",
    "final_preds = 0.25*(preds_a+preds_b+preds_c+preds_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ A smarter way to ensemble classifiers is to do a weighted average,where the weights are learned on the validation data--typicallly,the better classiifiters are given a higher weight,and the worse classifier are given a lower weight.In general,a simple weighted average with wieghts optimized on the validation data provides a very strong baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_a = model_a.predict(x_val)\n",
    "preds_b = model_b.predict(x_val)\n",
    "preds_c = model_c.predict(x_val)\n",
    "preds_d = model_d.predict(x_val)\n",
    "\n",
    "final_preds = 0.5*preds_a+0.25*preds_b+0.1*preds_c+0.15*preds_d)\n",
    "# These weights (0.5,0.25,0.1,0.15) are assumed to be learned empirically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The key to making ensembling work is the diversity of the set of classifiers.If your models are biased in different ways,the biases will cancel each other out,and the ensemble will be more robust and more accurate.\n",
    "+ For this reason,you should ensemble models that are as good as possible while being as different as possible.\n",
    "+ One thing that is largely not worth doing is ensembling the same network trained several times independently,from different random initializations.\n",
    "+ In recent times,one style of basic ensemble that has been very successful in practic is the wide and deep category of models,blending deep learning with shallow learning.Such models consist of jointly training a deep neural network with a large linear model.The joint training of a family of diverse models is yet another option to achieve model ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
