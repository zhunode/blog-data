{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This chapter explores deep-learning models that can process text(understood as sequences of word or sequences of characters),timeseries,and sequence data in general.\n",
    "+ The two fundamental deep-learning algorithms for sequence processing are recurrent neural networks and 1D convnets.\n",
    "+ Applications of these algorithms include the following:\n",
    "\n",
    ">+ Document classification and timeseries classification,such as identifying the topic of an article or the author of a book.\n",
    ">+ Timeseries comparisons,such as estimating how closely related two documents or two stock tickers are.\n",
    ">+ Sequence-tosequence learning,such as decoding an English sentence into French.\n",
    ">+ Sentitment analysis,such as classifying the sentiment of tweets or movie reviews as positive or negative.\n",
    ">+ Timeseries forecasting,such as predicting the future weather at a certain location,given recent weather data\n",
    "\n",
    "+ This chapter's examples focus on two narrow tasks:sentiment analysis on the IMDB dataset,a task we approached earlier in the book,and temperature forecating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with text data\n",
    "\n",
    "+ Text is one of the most widespread forms of sequence data.It can be understood as either a sequence of characters or a sequence of words,but it's most common to work at the level of words.\n",
    "+ Keep in mind througout this chapter that none of these deeplearning models truly understand text in a numan sense;rather,these models can map the statistical structure of written language,which is sufficient to solve many simple textual tasks.\n",
    "\n",
    "+ like all other neural networks,deep-learning models don't take as input raw text;they only work with numeric tensors.Vectorizing text is the process of transforming text into numeric tensorsThis can be done in multiple ways：\n",
    ">+ Segment text into words,and transform each word into a vector.\n",
    ">+ Segment text into characters,and transform each character into a vector.\n",
    ">+ Extract n-grams of words or characters,and transform each n-gram into a vector.N-grams are overlapping groups of multiple consecutive words or characters.\n",
    "\n",
    "+ The different units into which you can break down text(words,characters,or n-grams) are challed tokens,and breaking text into such tokens is called tokenization.\n",
    "+ This section will present two major ones:one-hot encoding of tokens,and token embedding(typically used exclusively for works and called word embedding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding n-grams and bag-of words\n",
    "+ Word n-grams are groups of N(or fewer) consecutive words that you can extract fromo a sentence.The same concept may also be applied to characters intead of words.\n",
    "+ Consider the sentence \"The cat sat on the mat.\" It may be decomposed into the following set of 2-grams:\n",
    ">+ {\"The\",\"The cat\",\"cat\",\"cat sat\",\"sat\",\"sat on\",\"on\",\"on the\",\"the\",\"the mat\",\"mat\"}\n",
    "\n",
    "+ It may also be decomposed into the following set of 3-grams:\n",
    ">+ {\"The\",\"The cat\",\"cat\",\"cat sat\",\"The cat sat\",\"sat\",\"sat on\",\"on\",\"cat sat on\",\"on the\",\"the\",\"sat on the\",\"the mat\",\"mat\",\"on the mat\"}\n",
    "\n",
    "+ Such a set is called a bag-of-2-grams or bag-of-3-grams.respectively.The term bag here refers to the fact that you're dealing with a set of tokens rather than a list or sequence:the tokens have no specific order.\n",
    "+ This family of tokenization methods is called bag-of-words.\n",
    "+ Because bag-of-words isn't an order-preserving tokenization method(the tokens generated are understood as a set not a sequence,and the general structure of the sentences is lost),it tends to be used in shallow language-processing models rather than in deep-learning models.Extracting n-grams is a form of feature enginnering.\n",
    "+ It won't cover n-grams any further in this book.`But to deep in mind that they're a powerful unavoidable feature-engineering tool when using lightweight,shallow text-processing models such as logistic regression and random forests.`\n",
    "\n",
    "n-grams：也没有保留文本的顺序性，从而在深度学习中不会使用该方法，除非训练的是浅层的语言模型可以是使用的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
